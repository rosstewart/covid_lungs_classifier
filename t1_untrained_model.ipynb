{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as torch_dataloader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as tv_models\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move data into train, val, test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "files_moved = 0\n",
    "if not os.path.exists('data/train/0/Non-Covid (1014).png'):\n",
    "    for tvt in ['train','val','test']:\n",
    "        with open(f'S224/{tvt}.csv','r') as f:\n",
    "            for line in f:\n",
    "                if line[0] == 'f':\n",
    "                    continue\n",
    "                line = line.replace('\\n','')\n",
    "                path,label = line.split(',')\n",
    "                label = int(float(label))\n",
    "                os.rename(f'S224/{path}',f\"data/{tvt}/{label}/{path.split('/')[-1]}\")\n",
    "                files_moved += 1\n",
    "print(files_moved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataloaders\n",
    "Reformatted data to work with dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "# #         transforms.RandomResizedCrop(224),\n",
    "# #         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.Resize(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# dataset_train = datasets.ImageFolder('data/train', transform=data_transforms['train'])\n",
    "# dataset_val = datasets.ImageFolder('data/val', transform=data_transforms['val'])\n",
    "dataset_test = datasets.ImageFolder('data/test', transform=data_transforms['test'])\n",
    "# loader_train = torch_dataloader(dataset_train, batch_size=8, shuffle=True, num_workers=0)\n",
    "# loader_val = torch_dataloader(dataset_val, batch_size=8, shuffle=False, num_workers=0) \n",
    "loader_test = torch_dataloader(dataset_test, batch_size=8, shuffle=False, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x,label)=dataset_train[0]\n",
    "# print(x.shape)\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x,label)=dataset_train[1000]\n",
    "# print(x.shape)\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(3, 3))\n",
    "# for n in range(1000, 1010, 1):\n",
    "#     x = dataset_train[n][0].detach().cpu().numpy()\n",
    "#     y = dataset_train[n][1]\n",
    "#     x = x.transpose(1,2,0)\n",
    "#     ax.imshow(x)\n",
    "#     ax.set_title('label: ' + str(y), fontsize=16)\n",
    "#     ax.axis('off')\n",
    "#     display.clear_output(wait=False)\n",
    "#     display.display(fig)\n",
    "#     plt.pause(0.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "# tv_models.resnet50(weights=ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CNN based on Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        #use resnet50 as the base model\n",
    "        #self.resnet50 = tv_models.resnet50(pretrained=True) #old Pytorch\n",
    "        self.resnet50 = tv_models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        #modified the last layer for binary classification  \n",
    "        self.resnet50.fc=torch.nn.Linear(2048, 1) #new layer\n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = self.resnet50(x)\n",
    "        z = z.view(-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_checkpoint(filename, model, optimizer, result, epoch):\n",
    "#     torch.save({'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'result':result},\n",
    "#                filename)\n",
    "#     print('saved:', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function to train the model in one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, optimizer, dataloader, epoch):    \n",
    "#     model.train()#set model to training mode\n",
    "#     loss_train=0\n",
    "#     acc_train =0 \n",
    "#     for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "#         #print(X.shape, Y.shape)\n",
    "#         #print(X.dtype, Y.dtype)\n",
    "#         Y = Y.to(X.dtype)\n",
    "#         X, Y = X.to(device), Y.to(device)\n",
    "#         Z = model(X)#forward pass\n",
    "#         loss = nnF.binary_cross_entropy_with_logits(Z, Y)\n",
    "#         optimizer.zero_grad()#clear grad of each parameter\n",
    "#         loss.backward()#backward pass\n",
    "#         optimizer.step()#update parameters\n",
    "#         loss_train+=loss.item()        \n",
    "#         Yp = (Z.data > 0).to(torch.int64)\n",
    "#         Y = Y.to(torch.int64)\n",
    "#         acc_train+= torch.sum(Yp==Y).item()\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print('Train Epoch: {} [{:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, 100. * batch_idx / len(dataloader), loss.item()))\n",
    "#     loss_train/=len(dataloader)\n",
    "#     acc_train/=len(dataloader.dataset) \n",
    "#     return loss_train, acc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Function to test the model\n",
    "It will calculate average accuracy, sensitivity(recall) and precision from the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, dataloader):\n",
    "    model.eval()#set model to evaluation mode\n",
    "    loss_test=0\n",
    "    acc_test =0\n",
    "    Confusion=np.zeros((2,2))\n",
    "    with torch.no_grad(): # tell Pytorch not to build graph in the with section\n",
    "        for batch_idx, (X, Y) in enumerate(dataloader):     \n",
    "            Y = Y.to(X.dtype)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            Z = model(X)#forward pass            \n",
    "            loss = nnF.binary_cross_entropy_with_logits(Z, Y)\n",
    "            loss_test+=loss.item()\n",
    "            Yp = (Z.data > 0).to(torch.int64)\n",
    "            Y = Y.to(torch.int64)\n",
    "            acc_test+= torch.sum(Yp==Y).item()\n",
    "            for i in range(0, 2):\n",
    "                for j in range(0, 2):\n",
    "                    Confusion[i,j]+=torch.sum((Y==i)&(Yp==j)).item()\n",
    "    loss_test/=len(dataloader)        \n",
    "    acc_test/=len(dataloader.dataset)\n",
    "    Sens=np.zeros(2)\n",
    "    Prec=np.zeros(2)   \n",
    "    for n in range(0, 2):\n",
    "        TP=Confusion[n,n]\n",
    "        FN=np.sum(Confusion[n,:])-TP\n",
    "        FP=np.sum(Confusion[:,n])-TP\n",
    "        Sens[n]=TP/(TP+FN)\n",
    "        Prec[n]=TP/(TP+FP)    \n",
    "    Acc = Confusion.diagonal().sum()/Confusion.sum() # should be the same as acc_test\n",
    "    return loss_test, acc_test, (Confusion, Acc, Sens, Prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model, and start the training-validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "model=Net()\n",
    "model.to(device)\n",
    "#optimizer = optim.SGD(model.get_trainable_parameters(), lr=0.0001, momentum=0.99) \n",
    "# optimizer = optim.Adam(model.get_trainable_parameters(), lr=0.0001) \n",
    "#---------------------------------------------------------\n",
    "(x,label)=dataset_test[0]\n",
    "x=x.view(1,3,224,224).to(device)\n",
    "z=model(x)\n",
    "#\n",
    "#run this whenever creating a new model\n",
    "# loss_train_list=[]\n",
    "# acc_train_list=[]\n",
    "# loss_val_list=[]\n",
    "# acc_val_list=[]\n",
    "# epoch_save=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0221], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4945], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.sigmoid(z)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_result(loss_train_list, acc_train_list, \n",
    "#                 loss_val_list, acc_val_list):    \n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "#     ax[0].set_title('loss v.s. epoch',fontsize=16)\n",
    "#     ax[0].plot(loss_train_list, '-b', label='training loss')\n",
    "#     ax[0].plot(loss_val_list, '-g', label='validation loss')\n",
    "#     ax[0].set_xlabel('epoch',fontsize=16)\n",
    "#     #ax[0].set_xticks(np.arange(len(loss_train_list)))\n",
    "#     ax[0].legend(fontsize=16)\n",
    "#     ax[0].grid(True)\n",
    "#     ax[1].set_title('accuracy v.s. epoch',fontsize=16)\n",
    "#     ax[1].plot(acc_train_list, '-b', label='training accuracy')\n",
    "#     ax[1].plot(acc_val_list, '-g', label='validation accuracy')\n",
    "#     ax[1].set_xlabel('epoch',fontsize=16)\n",
    "#     #ax[1].set_xticks(np.arange(len(loss_train_list)))\n",
    "#     ax[1].legend(fontsize=16)\n",
    "#     ax[1].grid(True)\n",
    "#     return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to update learning rate during training\n",
    "#lr_new=0.0001\n",
    "#for g in optimizer.param_groups:\n",
    "#    g['lr']=lr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best_acc_val = -99999\n",
    "# for epoch in range(epoch_save+1, epoch_save+30):\n",
    "#     t0=time.time()\n",
    "#     #-------- training --------------------------------\n",
    "#     loss_train, acc_train =train(model, device, optimizer, loader_train, epoch)    \n",
    "#     loss_train_list.append(loss_train)\n",
    "#     acc_train_list.append(acc_train)\n",
    "#     print('epoch', epoch, 'training loss:', loss_train, 'acc:', acc_train)\n",
    "#     #-------- validation --------------------------------\n",
    "#     loss_val, acc_val, other_val = test(model, device, loader_val)\n",
    "#     loss_val_list.append(loss_val)\n",
    "#     acc_val_list.append(acc_val)\n",
    "#     print('epoch', epoch, 'validation loss:', loss_val, 'acc:', acc_val)   \n",
    "#     t1=time.time()\n",
    "#     print(\"time cost\", t1-t0)\n",
    "#     #--------save model-------------------------\n",
    "#     result = (loss_train_list, acc_train_list, \n",
    "#               loss_val_list, acc_val_list, other_val)\n",
    "#     if acc_val > best_acc_val:\n",
    "#         best_acc_val = acc_val\n",
    "#         save_checkpoint('t1_TL_best_1.pt', model, optimizer, result, epoch)\n",
    "#     epoch_save=epoch\n",
    "#     #------- show result ----------------------\n",
    "#     display.clear_output(wait=False)\n",
    "#     plt.close('all')\n",
    "#     fig, ax = plot_result(loss_train_list, acc_train_list, \n",
    "#                           loss_val_list, acc_val_list)\n",
    "#     display.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_result(loss_train_list, acc_train_list, \n",
    "#             loss_val_list, acc_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_id= np.array(acc_val_list).argmax()\n",
    "# best_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch_save=best_id\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# checkpoint=torch.load('t1_TL_best_1.pt', map_location=device)\n",
    "# model=Net()\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.to(device)\n",
    "model.eval() \n",
    "# #\n",
    "# optimizer = optim.Adam(model.get_trainable_parameters(), lr=0.0001) \n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# #\n",
    "# (loss_train_list, acc_train_list, \n",
    "#  loss_val_list, acc_val_list, other_val) = checkpoint['result']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET:\n",
      "\n",
      "Accuracy (average) 0.52\n",
      "Accuracy (average) 0.52\n",
      "Sensitivity [0.615 0.425]\n",
      "Precision [0.51680672 0.52469136]\n",
      "Confusion_sens \n",
      " [[0.615 0.385]\n",
      " [0.575 0.425]]\n",
      "Confusion_prec \n",
      " [[0.51680672 0.47530864]\n",
      " [0.48319328 0.52469136]]\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test, (Confusion, Acc, Sens, Prec) = test(model, device, loader_test)\n",
    "Confusion_sens=Confusion.copy()\n",
    "for n in range(0, 2):\n",
    "    Confusion_sens[n,:]/=np.sum(Confusion[n,:])\n",
    "Confusion_prec=Confusion.copy()\n",
    "for n in range(0, 2):\n",
    "    Confusion_prec[:,n]/=np.sum(Confusion[:,n])\n",
    "print('TEST SET:\\n')\n",
    "print('Accuracy (average)', acc_test)\n",
    "print('Accuracy (average)', Acc)\n",
    "print('Sensitivity', Sens)\n",
    "print('Precision', Prec)\n",
    "print('Confusion_sens \\n', Confusion_sens)\n",
    "print('Confusion_prec \\n', Confusion_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.8_csc546_hw5s_env)",
   "language": "python",
   "name": "3.8_csc546_hw5s_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
